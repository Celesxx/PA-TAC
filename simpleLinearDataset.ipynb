{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from script.plotDisplayLinear import plotDisplay\n",
    "from ctypes import c_char_p\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = ctypes.CDLL(\"modele/linear/target/release/liblinear_classification.so\")\n",
    "\n",
    "\n",
    "linear_model.LM_init.argtypes = [\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_char\n",
    "]\n",
    "linear_model.LM_init.restype = ctypes.POINTER(ctypes.c_void_p)\n",
    "\n",
    "linear_model.LM_free.argtypes = [ctypes.POINTER(ctypes.c_void_p)]\n",
    "\n",
    "linear_model.LM_train.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_void_p),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_size_t\n",
    "]\n",
    "\n",
    "linear_model.LM_predict.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_void_p),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.jit(nopython=True)\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        # img = Image.open(img_path).convert('L')  # Conversion en gris\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize((32, 32))  # Resize les images\n",
    "        img_array = np.array(img, dtype=np.float64).flatten()  # Flatten les vecteurs\n",
    "        images.append(img_array)\n",
    "        labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "folder_a = 'data/dataset/resized/gas_giant'\n",
    "folder_b = 'data/dataset/resized/neptune-like'\n",
    "folder_c = 'data/dataset/resized/super_earth'\n",
    "\n",
    "images_a, labels_a = load_images_from_folder(folder_a, [1.0, 0.0, 0.0])\n",
    "images_b, labels_b = load_images_from_folder(folder_b, [0.0, 1.0, 0.0])\n",
    "images_c, labels_c = load_images_from_folder(folder_c, [0.0, 0.0, 1.0])\n",
    "\n",
    "X = np.array(images_a + images_b + images_c)\n",
    "y = np.array(labels_a + labels_b + labels_c)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train)\n",
    "X_train_std = np.std(X_train)\n",
    "\n",
    "print(X_train[5])\n",
    "print(X_test[:5])\n",
    "\n",
    "x_train = (X_train - X_train_mean) / X_train_std\n",
    "x_test = (X_test - X_train_mean) / X_train_std\n",
    "print(\"-----------------\")\n",
    "print(X_train[5])\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR test\n",
    "learning_rate = 0.05\n",
    "weights = np.array([0.1, -0.4, 0.6, -0.24], dtype=np.float64, order='C')\n",
    "bias = 0.2\n",
    "epochs = 10\n",
    "activation = b't'\n",
    "# Initialisation du modèle\n",
    "model = linear_model.LM_init(learning_rate, weights, weights.size, bias, activation)\n",
    "\n",
    "\n",
    "n_samples, n_features = x_train.shape[0], x_train.shape[1]\n",
    "\n",
    "linear_model.LM_train(model, x_train.flatten(), y_train.flatten(), n_samples, n_features, epochs)\n",
    "\n",
    "predictions = np.zeros(n_samples, dtype=np.float64)\n",
    "\n",
    "linear_model.LM_predict(\n",
    "    model,\n",
    "    x_train,\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    predictions\n",
    ")\n",
    "\n",
    "print(\"---------------\")\n",
    "print(predictions)\n",
    "print(\"---------------\")\n",
    "\n",
    "linear_model.LM_free(model)\n",
    "\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(x_train[:, 0], x_train[:, 1], y_train, color='blue', label='Données réelles')\n",
    "# ax.scatter(x_train[:, 0], x_train[:, 1], predictions, color='red', label='Prédictions')\n",
    "# ax.set_xlabel('X1')\n",
    "# ax.set_ylabel('X2')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear multi class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
